{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    def forward (self, x):\n",
    "        \"\"\"\n",
    "            Y = ReLU(x)\n",
    "            \n",
    "            parameters:\n",
    "                * x: input of relu\n",
    "                    * size: (batch_size, in_dim)\n",
    "                    \n",
    "            output:\n",
    "                * y: y = relu(x)\n",
    "                    * size: (batch_size, in_dim) \n",
    "        \"\"\"\n",
    "        return np.maximum(0,x)\n",
    "    \n",
    "    def backward (self, x):\n",
    "        \"\"\"\n",
    "            dY/dx \n",
    "            \n",
    "            parameters:\n",
    "                * X: input of relu\n",
    "                    * size: (batch_size, in_dim)\n",
    "                    \n",
    "            outputs:\n",
    "                * dy: dy/dx\n",
    "                    * size: (batch_size, in_dim)\n",
    "        \"\"\"\n",
    "        dy = np.array(x, dtype=float)\n",
    "        dy[dy>0]=1 \n",
    "        dy[dy==0]=0.5\n",
    "        dy[dy<0]=0\n",
    "       \n",
    "        return dy\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"ReLU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU\n"
     ]
    }
   ],
   "source": [
    "relu = ReLU()\n",
    "print(relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "   \n",
    "   def __init__(self,in_dim,out_dim, act_func) -> None:\n",
    "      self.weight = np.random.randn(out_dim , in_dim+1) * np.sqrt(2/(in_dim +out_dim ))\n",
    "      self.act_func = act_func\n",
    "      self.in_dim = in_dim\n",
    "      self.out_dim = out_dim\n",
    "      self.score = np.zeros (out_dim)\n",
    "      self.activated = np.zeros (out_dim)\n",
    "      \n",
    "      \n",
    "   def forward (self, x):\n",
    "      x_b = np.ones((x.shape[0], self.in_dim+1))\n",
    "      x_b[: , :self.in_dim] = x\n",
    "      \n",
    "      self.score = np.dot(self.weight , x_b.T).T\n",
    "      self.activated = self.act_func.forward(self.score)\n",
    "      return self.activated\n",
    "   \n",
    "   def backward (self, dloss_da , activated_prev):\n",
    "      \n",
    "      da_ds = self.act_func.backward(self.score)\n",
    "      \n",
    "      dloss_ds = np.multiply (dloss_da, da_ds)\n",
    "      \n",
    "      ds_dw = activated_prev\n",
    "      \n",
    "      dloss_dw = np.dot (dloss_ds.T, ds_dw)\n",
    "      \n",
    "      dloss_da_prev = np.dot (dloss_ds,self.weight)\n",
    "      \n",
    "      dloss_da_prev = dloss_da_prev[:, :-1]\n",
    "      \n",
    "      return dloss_dw, dloss_da_prev\n",
    "   \n",
    "   def update(self,dloss_dw, lr):\n",
    "      self.weight = self.weight - np.multiply(lr, dloss_dw)\n",
    "   \n",
    "   def __repr__(self):\n",
    "      return f\"LinearLayer(in_dim={self.in_dim}, out_dim={self.out_dim}, activation={self.act_func})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 5), dtype('float64'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([\n",
    "    [-10, 5, 0, -5, 10],\n",
    "    [4, -1, 0, 1, -4],\n",
    "    [-5 , -3, 0, 3, 6]\n",
    "], dtype=np.float64)\n",
    "\n",
    "X.shape, X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = ReLU()\n",
    "linear_layer = LinearLayer(\n",
    "    in_dim = 5,\n",
    "    out_dim = 4,\n",
    "    act_func = relu,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearLayer(in_dim=5, out_dim=4, activation=ReLU)\n"
     ]
    }
   ],
   "source": [
    "print(linear_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test `.forward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer.forward(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test `.backward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "activated_prev = np.array([\n",
    "    [-10, 5, 0, -5, 10],\n",
    "    [4, -1, 0, 1, -4],\n",
    "    [-5 , -3, 0, 3, 6]\n",
    "], dtype=np.float64)\n",
    "\n",
    "dloss_da = np.array([\n",
    "    [-10, 5, 0, 2],\n",
    "    [4, -1, 1, 3],\n",
    "    [-5 , -3, 6, 4]\n",
    "], dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 16.,  -4.,   0.,   4., -16.],\n",
       "        [-35.,  34.,   0., -34.,  32.],\n",
       "        [  0.,   0.,   0.,   0.,   0.],\n",
       "        [ -8., -15.,   0.,  15.,  12.]]),\n",
       " array([[-0.9935543 , -0.35606727,  5.03234059,  1.24950395,  0.54719047],\n",
       "        [ 1.79389205, -0.62453631,  1.43614817,  2.37924421, -0.47955066],\n",
       "        [ 0.16321562,  0.60394208, -2.36083875,  1.98477711, -0.97724301]]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer.backward(dloss_da, activated_prev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__ (self,n_nodes, act_func_list, lr):\n",
    "        self.n_nodes = n_nodes\n",
    "        self.act_func_list = act_func_list\n",
    "        self.lr = lr\n",
    "        self.layers = self.get_layers()\n",
    "        \n",
    "        \n",
    "    def get_layers(self):\n",
    "        layers = []\n",
    "        for i in range(len(self.act_func_list)):\n",
    "            in_dim = self.n_nodes[i]\n",
    "            out_dim = self.n_nodes[i+1]\n",
    "            act_func = self.act_func_list[i]\n",
    "            \n",
    "            layer = LinearLayer(in_dim ,out_dim,act_func)\n",
    "            layers.append(layer)\n",
    "                \n",
    "        return layers\n",
    "   \n",
    "    def forward(self,x):\n",
    "        y_current = x\n",
    "        for i in range (len(self.layers) ):\n",
    "           y_next = self.layers[i].forward(y_current)\n",
    "           y_current = y_next\n",
    "           \n",
    "        return y_current\n",
    "    \n",
    "    def backward(self, dloss_dy):\n",
    "        dloss_da = dloss_dy\n",
    "        for i in reversed(range(len(self.act_func_list))):\n",
    "            a_prev = self.layers[i-1].activated\n",
    "            \n",
    "            dloss_dw, dloss_da_prev = self.layers[i].backward(dloss_da , a_prev)\n",
    "            dloss_da = dloss_da_prev\n",
    "            \n",
    "            self.layers[i].update(dloss_dw,self.lr)\n",
    "    \n",
    "    def __repr__ (self):\n",
    "        mess = ''\n",
    "        for i in range (len(self.act_func_list)):\n",
    "            # self.layers[i]\n",
    "            # mess= mess + f\"{i}: LinearLayer(in_dim={self.n_nodes[i]}, out_dim={self.n_nodes[i+1]},activation={self.act_func_list[i]})\\n\"\n",
    "            mess = mess + f\"{i}: {self.layers[i]}\\n\"\n",
    "        return mess\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in reversed(range(10)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = [128, 64, 40 ,16 ,8, 3]\n",
    "relu = ReLU()\n",
    "act_func_list = [relu , relu ,relu,relu,relu]\n",
    "lr = 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(n_nodes , act_func_list, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: LinearLayer(in_dim=128, out_dim=64, activation=ReLU)\n",
      "1: LinearLayer(in_dim=64, out_dim=40, activation=ReLU)\n",
      "2: LinearLayer(in_dim=40, out_dim=16, activation=ReLU)\n",
      "3: LinearLayer(in_dim=16, out_dim=8, activation=ReLU)\n",
      "4: LinearLayer(in_dim=8, out_dim=3, activation=ReLU)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test `.forward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 128)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.rand(4, 128)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = mlp.forward(X)\n",
    "\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LinearLayer(in_dim=128, out_dim=64, activation=ReLU),\n",
       " LinearLayer(in_dim=64, out_dim=40, activation=ReLU),\n",
       " LinearLayer(in_dim=40, out_dim=16, activation=ReLU),\n",
       " LinearLayer(in_dim=16, out_dim=8, activation=ReLU),\n",
       " LinearLayer(in_dim=8, out_dim=3, activation=ReLU)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReLU"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"e:\\UT\\Summer 02\\Deep Learning (Reshad)\\HWs\\01\\codes\\test.py\", line 8, in <module>\n",
      "    relu(relu.forward(np.random.randn(3, 4)))\n",
      "TypeError: 'ReLU' object is not callable\n"
     ]
    }
   ],
   "source": [
    "!python ./test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
